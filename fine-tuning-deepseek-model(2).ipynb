{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credits\n\nLearnt this from Abid Ali Awan's tutorial on Fine-Tuning DeepSeek R1, posted on DataCamp\n\n## Packages & Tools used\n- `unsloth`: Fine-Tuning for LLMs\n    - `FastLanguageModel` for optimizing Fine-Tuning & inference\n- `peft` for LoRa (Low Rank Adapdation)\n    - \"LoRa\" fine-tunes *some* of the parameters, making it efficient for weaker GPUs\n- Hugging Face Models like:\n    - `transformers` works with the dataset, handles model work\n    - `trl` Transformer Reinforcement Learning gives supervised fine-tuning learning & uses the `SFFTrainer` wrapper\n    - `datasets` to access the dataset i'm using from HuggingFace\n- `torch` DL framework\n- `wandb` Weights & Biases, tracks the fine-tuning \n","metadata":{}},{"cell_type":"code","source":"# %%capture\n\n# !pip install unsloth # install unsloth\n# print('installed unsloth')\n# print('installed')\n# !pip install unsloth\n!pip install unsloth\n# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:27.051646Z","iopub.execute_input":"2025-06-19T21:22:27.051922Z","iopub.status.idle":"2025-06-19T21:22:30.274198Z","shell.execute_reply.started":"2025-06-19T21:22:27.051902Z","shell.execute_reply":"2025-06-19T21:22:30.273413Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.6.2)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!\nprint('install done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:30.275891Z","iopub.execute_input":"2025-06-19T21:22:30.276159Z","iopub.status.idle":"2025-06-19T21:22:40.186562Z","shell.execute_reply.started":"2025-06-19T21:22:30.276135Z","shell.execute_reply":"2025-06-19T21:22:40.185436Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting git+https://github.com/unslothai/unsloth.git\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-grlhkvdq\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-grlhkvdq\n  Resolved https://github.com/unslothai/unsloth.git to commit 37e577a91386cb5b4a7b818a1418b66beef17296\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for unsloth: filename=unsloth-2025.6.2-py3-none-any.whl size=278541 sha256=f147931fbe941a7572908163ec191229fcf4b86ab37fb30e9cdaa22eb5bee072\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wv6tfvil/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\nSuccessfully built unsloth\nInstalling collected packages: unsloth\n  Attempting uninstall: unsloth\n    Found existing installation: unsloth 2025.6.2\n    Uninstalling unsloth-2025.6.2:\n      Successfully uninstalled unsloth-2025.6.2\nSuccessfully installed unsloth-2025.6.2\ninstall done\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\nprint('setup complete')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:40.187594Z","iopub.execute_input":"2025-06-19T21:22:40.187882Z","iopub.status.idle":"2025-06-19T21:22:40.194210Z","shell.execute_reply.started":"2025-06-19T21:22:40.187856Z","shell.execute_reply":"2025-06-19T21:22:40.193278Z"}},"outputs":[{"name":"stdout","text":"setup complete\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"    # Initialize Hugging Face & WnB tokens\n    user_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\n    hugging_face_token = user_secrets.get_secret(\"Hugging_Face_Token\")\n    wnb_token = user_secrets.get_secret(\"wnb\")\n    \n    # Login to Hugging Face\n    login(hugging_face_token) # from huggingface_hub import login\n    \n    # Login to WnB\n    wandb.login(key=wnb_token) # import wandb\n    run = wandb.init(\n        project='Fine-tune-DeepSeek-R1-Distill-Llama-8B on Medical COT Dataset', \n        job_type=\"training\", \n        anonymous=\"allow\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:40.197033Z","iopub.execute_input":"2025-06-19T21:22:40.197341Z","iopub.status.idle":"2025-06-19T21:22:47.111111Z","shell.execute_reply.started":"2025-06-19T21:22:40.197315Z","shell.execute_reply":"2025-06-19T21:22:47.110507Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250619_212240-5kuilea6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5kuilea6?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c' target=\"_blank\">smooth-vortex-11</a></strong> to <a href='https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c' target=\"_blank\">https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5kuilea6?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c' target=\"_blank\">https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5kuilea6?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"# Loading DeepSeek & Tokenizer\n`FastLanguageModel.from_pretrained()` loads DeepSeek & its tokenizer. Using a distilled 8B version of R1 for faster computation. Also, configuration of parameters in code is explained. \n\n**4-bit quantization**\nEnabling 4bQ allows for improved memory saving - important for limited GPUs. 4bQ reduces the precision of model weights, whilst keeping most of the accuracy. This allows it to be smalled & faster. \n\n4Bq compresses numbers into 4 bit values (instead of 32 / 16 bits), which is key to running models on normal GPUs. ","metadata":{}},{"cell_type":"code","source":"max_seq_length = 2048 # max seq. length of model (i.e max no. of tokens processed)\ndtype = None # defautls to floating-point 16 (i think?)\nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Load the DeepSeek R1 model (8B parameter version)\n    max_seq_length=1024, # model processes 2048 sequeces/tokens\n    dtype=dtype, # defaults to FP16 or BF16 \n    load_in_4bit=load_in_4bit, \n    token=hugging_face_token,\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:47.111922Z","iopub.execute_input":"2025-06-19T21:22:47.112163Z","iopub.status.idle":"2025-06-19T21:22:56.863540Z","shell.execute_reply.started":"2025-06-19T21:22:47.112146Z","shell.execute_reply":"2025-06-19T21:22:56.862952Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Example of response before Fine-Tuning\n\n1) Define a system prompt  \n2) Running an Inference on the Model  \n","metadata":{}},{"cell_type":"code","source":"# Define a system prompt under prompt_style \nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"\n\n\n# Creating a test medical question for inference\nquestion = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or \n              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n\n# Format the question using the structured prompt (`prompt_style`) and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  # Convert input to PyTorch tensor & move to GPU\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids=inputs.input_ids, # Tokenized input question\n    attention_mask=inputs.attention_mask, # Attention mask to handle padding\n    max_new_tokens=1200, # Limit response length to 1200 tokens (to prevent excessive output)\n    use_cache=True, # Enable caching for faster inference\n)\n\n# Decode the generated output tokens into human-readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the relevant response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:22:56.864193Z","iopub.execute_input":"2025-06-19T21:22:56.864370Z","iopub.status.idle":"2025-06-19T21:23:32.549608Z","shell.execute_reply.started":"2025-06-19T21:22:56.864356Z","shell.execute_reply":"2025-06-19T21:23:32.548852Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so I'm trying to figure out what cystometry would show for this 61-year-old woman. Let me break this down.\n\nFirst, the patient has a history of involuntary urine loss when she coughs or sneezes, but she doesn't leak at night. That makes me think she might have some kind of bladder control issue, maybe related to the lower urinary tract. Since it happens during activities that put pressure on the bladder, like coughing, it's probably due to an overactive bladder or maybe some nerve issue affecting the bladder's control.\n\nShe underwent a gynecological exam and a Q-tip test. I'm not too familiar with the Q-tip test, but I think it's used to check for urethral obstruction. If the Q-tip is placed in the urethra and stays there, it suggests that the urethral opening is narrow or blocked, leading to difficulty in voiding. So if the Q-tip test came back positive, that would mean there's obstruction at the urethral level.\n\nNow, considering the findings, what would cystometry reveal? Cystometry is a test where they fill the bladder and measure how it holds and releases. It can show things like how much the bladder can hold (residual volume) and whether there are detrusor contractions, which are the involuntary muscle contractions that push urine out.\n\nSince she has involuntary loss, maybe her detrusor muscle is overactive, leading to frequent small contractions. Alternatively, if there's urethral obstruction, the detrusor contractions might be stronger or more frequent because the outlet is blocked, so the bladder has to contract harder to push the urine out. But in her case, she doesn't leak at night, so maybe her night-time detrusor activity isn't causing leakage, perhaps because she's asleep and doesn't contract enough.\n\nPutting this together, the cystometry would likely show an increased residual volume. That makes sense because if the detrusor is overactive, the bladder might empty partially, but not completely, leading to residual urine. Also, there might be more detrusor contractions, indicating that the bladder is contracting involuntarily more than usual.\n\nWait, but if the Q-tip test was positive, does that affect the residual volume? Maybe. If there's urethral obstruction, the bladder might have to contract harder to expel urine, which could lead to more detrusor contractions. But the residual volume would depend on how much is left after those contractions. If the detrusor is overactive, maybe the bladder doesn't hold as much as normal, leading to lower residual volume. But I'm a bit confused here.\n\nAlternatively, maybe the residual volume is increased because the detrusor contractions are frequent, and the bladder doesn't empty completely. So, during cystometry, they would measure a higher residual volume and more detrusor contractions.\n\nI think the key points are that the Q-tip test indicates urethral obstruction, and the history of involuntary loss suggests detrusor overactivity. So, the cystometry would show an increased residual volume due to frequent small contractions and possibly more detrusor contractions, indicating overactivity.\n</think>\n\nCystometry for this 61-year-old woman would most likely reveal an increased residual volume and detrusor contractions. The Q-tip test indicating urethral obstruction suggests that the detrusor contractions may be more frequent or forceful, as the bladder must contract harder to overcome the obstruction. Additionally, the history of involuntary urine loss during activities such as coughing, along with no leakage at night, points to detrusor overactivity. This would result in the bladder not fully emptying, leading to residual urine and increased detrusor contractions during the test. \n\n**Answer:** Cystometry would most likely reveal an increased residual volume and detrusor contractions.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Issues w/ response\n\nThe reasoning process is showcased in between the `<think></think>`\n\nThis response has many issues. The reasoning process, while detailed, was long-winded and not concise. Furthermore, we want the final answer to be consistent in a certain style. ","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning the model \n\nStep 1: Update the system prompt to include `<think></think>`","metadata":{}},{"cell_type":"code","source":"# Updated training prompt style to add </think> tag \ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:32.550491Z","iopub.execute_input":"2025-06-19T21:23:32.551206Z","iopub.status.idle":"2025-06-19T21:23:32.555567Z","shell.execute_reply.started":"2025-06-19T21:23:32.551180Z","shell.execute_reply":"2025-06-19T21:23:32.554852Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"Step 2: Using the medical Medical O1 Reasoninng SFT ([HuggingFace Link](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)) to fine-tune our model.\n\nOriginally, this model was used to fine-tune HuatuoGPT-o1, a medical LLM. \n","metadata":{}},{"cell_type":"code","source":"# Download the dataset using Hugging Face — function imported using from datasets import load_dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:500]\",trust_remote_code=True) # 500 rows for train, the rest for test/validation\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:32.556322Z","iopub.execute_input":"2025-06-19T21:23:32.556589Z","iopub.status.idle":"2025-06-19T21:23:33.388740Z","shell.execute_reply.started":"2025-06-19T21:23:32.556567Z","shell.execute_reply":"2025-06-19T21:23:33.388126Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"dataset[499]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:33.389442Z","iopub.execute_input":"2025-06-19T21:23:33.389650Z","iopub.status.idle":"2025-06-19T21:23:33.395932Z","shell.execute_reply.started":"2025-06-19T21:23:33.389634Z","shell.execute_reply":"2025-06-19T21:23:33.395272Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'Question': 'What type of contrast agent is recommended for use in a patient with renal dysfunction to prevent contrast nephropathy?',\n 'Complex_CoT': \"Okay, so let's figure this out. Contrast nephropathy is something to be cautious about, especially when we're dealing with patients with kidney issues. This condition happens after using contrast agents during imaging procedures, and it can lead to acute kidney injury. It sounds pretty serious, so we definitely want to choose the safest option for these patients.\\n\\nThere are several types of contrast media available, and they come in different osmolarities. We've got high-osmolar contrast media (HOCM), low-osmolar contrast media (LOCM), and then there's iso-osmolar contrast media (IOCM). From what I remember, high-osmolar agents aren't really great for people with kidney problems because they tend to have a higher risk of causing nephropathy.\\n\\nOh, for patients with renal dysfunction, we need to be extra careful. We should opt for contrast agents that are gentler on the kidneys. Here, iso-osmolar contrast media, like iodixanol, come into play. They've been shown to be less risky in terms of inducing nephropathy, especially for those high-risk patients we’re thinking about.\\n\\nSo, based on that knowledge, I feel pretty confident saying that if we're dealing with someone with renal dysfunction, the iso-osmolar contrast agents like iodixanol are the way to go. They really seem to minimize the risk, and that's exactly what we want here.\\n\\nJust to make sure we're on the right track, looking through clinical guidelines and research backs up this thought. Yup, iso-osmolar contrast media are preferred due to their lower association with kidney damage. This all checks out.\",\n 'Response': 'Iso-osmolar contrast agents, such as iodixanol, are recommended for use in patients with renal dysfunction to help prevent contrast nephropathy. These agents are preferred due to their lower risk of inducing kidney damage compared to high-osmolar or low-osmolar contrast media, making them a safer choice for high-risk patients.'}"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"Each question has a \"chain of thought\" and a response.  \nThis dataset format ensures every training example follows a consistent pattern  \nWe're also gonna prevent the model from yapping tm by adding an EOS (end of sentence?) token ","metadata":{}},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training\nEOS_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:33.397910Z","iopub.execute_input":"2025-06-19T21:23:33.398166Z","iopub.status.idle":"2025-06-19T21:23:33.412443Z","shell.execute_reply.started":"2025-06-19T21:23:33.398150Z","shell.execute_reply":"2025-06-19T21:23:33.411871Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'<｜end▁of▁sentence｜>'"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# Define formatting prompt function\ndef formatting_prompts_func(examples):  # Takes a batch of dataset examples as input\n    inputs = examples[\"Question\"]       # Extracts the medical question from the dataset\n    cots = examples[\"Complex_CoT\"]      # Extracts the chain-of-thought reasoning (logical step-by-step explanation)\n    outputs = examples[\"Response\"]      # Extracts the final model-generated response (answer)\n    \n    texts = []  # Initializes an empty list to store the formatted prompts\n    \n    # Iterate over the dataset, formatting each question, reasoning step, and response\n    for input, cot, output in zip(inputs, cots, outputs):  \n        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN  # Insert values into prompt template & append EOS token\n        texts.append(text)  # Add the formatted text to the list\n\n    return {\n        \"text\": texts,  # Return the newly formatted dataset with a \"text\" column containing structured prompts\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:33.413213Z","iopub.execute_input":"2025-06-19T21:23:33.413829Z","iopub.status.idle":"2025-06-19T21:23:33.428872Z","shell.execute_reply.started":"2025-06-19T21:23:33.413804Z","shell.execute_reply":"2025-06-19T21:23:33.428275Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Update dataset formatting\ndataset_finetune = dataset.map(formatting_prompts_func, batched = True)\ndataset_finetune[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:33.429616Z","iopub.execute_input":"2025-06-19T21:23:33.429827Z","iopub.status.idle":"2025-06-19T21:23:33.452204Z","shell.execute_reply.started":"2025-06-19T21:23:33.429811Z","shell.execute_reply":"2025-06-19T21:23:33.451454Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Response:\\n<think>\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n</think>\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.<｜end▁of▁sentence｜>\""},"metadata":{}}],"execution_count":40},{"cell_type":"markdown","source":"Step 3: Using LoRa  \nLoRA (**Low-Rank Adaptation**) will allow me to fine-tune this model efficiently. This is because a standard LLM has millions/billions of parameters that determine how to process/generate text. Fine-tuning requires updating all the weights, which requires too much resources.\n\nLoRa adds small, trainable adapters to specific layers, which capture task-specific knowledge whilst leaving the original model unchanged. This reduces the number of trainable parameters **by more than 90%**, making fine-tuning **faster and more memory-efficient**.  \n\nTutorial by [Sebastian Raschka](https://www.youtube.com/watch?v=rgmJep4Sb4&t).\n\nusing `get_peft_model()` function (Parameter-Efficient Fine-Tuning), this function wraps the base model (`model`) with LoRA modifications, ensuring that only specific parameters are trained.  ","metadata":{}},{"cell_type":"code","source":"# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model \nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)\n    target_modules=[  # List of transformer layers where LoRA adapters will be applied\n        \"q_proj\",   # Query projection in the self-attention mechanism\n        \"k_proj\",   # Key projection in the self-attention mechanism\n        \"v_proj\",   # Value projection in the self-attention mechanism\n        \"o_proj\",   # Output projection from the attention layer\n        \"gate_proj\",  # Used in feed-forward layers (MLP)\n        \"up_proj\",    # Part of the transformer’s feed-forward network (FFN)\n        \"down_proj\",  # Another part of the transformer’s FFN\n    ],\n    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)\n    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)\n    bias=\"none\",  # Specifies whether LoRA layers should learn bias terms (setting to \"none\" saves memory)\n    use_gradient_checkpointing=\"unsloth\",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)\n    random_state=3407,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs\n    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)\n    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:33.453013Z","iopub.execute_input":"2025-06-19T21:23:33.453265Z","iopub.status.idle":"2025-06-19T21:23:40.945322Z","shell.execute_reply.started":"2025-06-19T21:23:33.453239Z","shell.execute_reply":"2025-06-19T21:23:40.944760Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Now, we initialize SFTTrainer, a supervised fine-tuning trainer \n# from `trl` (Transformer Reinforcement Learning), \n# to fine-tune our model efficiently on a dataset.\n# Initialize the fine-tuning trainer — Imported using from trl import SFTTrainer\ntrainer = SFTTrainer(\n    model=model_lora,  # The model to be fine-tuned\n    tokenizer=tokenizer,  # Tokenizer to process text inputs\n    train_dataset=dataset_finetune,  # Dataset used for training\n    dataset_text_field=\"text\",  # Specifies which field in the dataset contains training text\n    max_seq_length=max_seq_length,  # Defines the maximum sequence length for inputs\n    dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing\n\n    # Define training arguments\n    args=TrainingArguments(\n        per_device_train_batch_size=2,  # Number of examples processed per device (GPU) at a time\n        gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps before updating weights\n        num_train_epochs=1, # Full fine-tuning run\n        warmup_steps=5,  # Gradually increases learning rate for the first 5 steps\n        max_steps=60,  # Limits training to 60 steps (useful for debugging; increase for full fine-tuning)\n        learning_rate=2e-4,  # Learning rate for weight updates (tuned for LoRA fine-tuning)\n        fp16=not is_bfloat16_supported(),  # Use FP16 (if BF16 is not supported) to speed up training\n        bf16=is_bfloat16_supported(),  # Use BF16 if supported (better numerical stability on newer GPUs)\n        logging_steps=10,  # Logs training progress every 10 steps\n        optim=\"adamw_8bit\",  # Uses memory-efficient AdamW optimizer in 8-bit mode\n        weight_decay=0.01,  # Regularization to prevent overfitting\n        lr_scheduler_type=\"linear\",  # Uses a linear learning rate schedule\n        seed=3407,  # Sets a fixed seed for reproducibility\n        output_dir=\"outputs\",  # Directory where fine-tuned model checkpoints will be saved\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:40.946322Z","iopub.execute_input":"2025-06-19T21:23:40.946573Z","iopub.status.idle":"2025-06-19T21:23:41.310807Z","shell.execute_reply.started":"2025-06-19T21:23:40.946556Z","shell.execute_reply":"2025-06-19T21:23:41.310110Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"Step 4: Model Training  \n","metadata":{}},{"cell_type":"code","source":"# fine-tuning process\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T21:23:41.311684Z","iopub.execute_input":"2025-06-19T21:23:41.311926Z","iopub.status.idle":"2025-06-19T22:05:05.936864Z","shell.execute_reply.started":"2025-06-19T21:23:41.311905Z","shell.execute_reply":"2025-06-19T22:05:05.936110Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 40:35, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.938100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.417800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.397500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.354700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.383600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.363300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()\nprint('finished!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T22:05:05.937816Z","iopub.execute_input":"2025-06-19T22:05:05.938098Z","iopub.status.idle":"2025-06-19T22:05:06.475269Z","shell.execute_reply.started":"2025-06-19T22:05:05.938074Z","shell.execute_reply":"2025-06-19T22:05:06.474697Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▄▅▇██</td></tr><tr><td>train/global_step</td><td>▁▂▄▅▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▁▁▂▁</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.6770438117310464e+16</td></tr><tr><td>train/epoch</td><td>0.96</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.25618</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3633</td></tr><tr><td>train_loss</td><td>1.47585</td></tr><tr><td>train_runtime</td><td>2481.8979</td></tr><tr><td>train_samples_per_second</td><td>0.193</td></tr><tr><td>train_steps_per_second</td><td>0.024</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">smooth-vortex-11</strong> at: <a href='https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5kuilea6?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c' target=\"_blank\">https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset/runs/5kuilea6?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c</a><br> View project at: <a href='https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c' target=\"_blank\">https://wandb.ai/lilsolar117/Fine-tune-DeepSeek-R1-Distill-Llama-8B%20on%20Medical%20COT%20Dataset?apiKey=9417cb8fd1468241a68d68853a3ecca58b3f280c</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250619_212240-5kuilea6/logs</code>"},"metadata":{}},{"name":"stdout","text":"finished!\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"Step 5: New model response after fine-tuning","metadata":{}},{"cell_type":"code","source":"question = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \n              but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, \n              what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T22:05:06.475961Z","iopub.execute_input":"2025-06-19T22:05:06.476278Z","iopub.status.idle":"2025-06-19T22:05:37.028484Z","shell.execute_reply.started":"2025-06-19T22:05:06.476255Z","shell.execute_reply":"2025-06-19T22:05:37.027624Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so let's break this down. We have a 61-year-old woman who's been dealing with involuntary urine loss during things like coughing or sneezing, but she's not having any problems at night. That's interesting because it usually points towards some kind of issue with her bladder or urethra, but not necessarily something that affects her sleep.\n\nWhen they did the gynecological exam and the Q-tip test, they found something. Now, I'm not entirely sure what they found, but I'm guessing it's not something that's super serious. The Q-tip test is usually done to see if there's a urethral obstruction or something like that. So if it's positive, it suggests there's an obstruction. If it's negative, then maybe it's more of a bladder issue.\n\nNow, the big question is what would cystometry show. Cystometry is like a test where they fill up the bladder and see how it behaves. If there's a residual volume that's more than usual, it could mean that her bladder isn't emptying completely. That's something to watch out for.\n\nAlso, they're going to check out how her detrusor muscle is contracting. The detrusor muscle is supposed to push urine out. If it's not working well, that's going to mean she's not able to fully empty her bladder. So, if there's a problem here, it's going to show up in the cystometry results.\n\nPutting it all together, if the Q-tip test was negative, it's more likely that the problem is with the bladder itself. So, we'd expect to see some issues with how it's contracting or how much it can hold before it empties. That's probably what they'll find when they do the cystometry.\n\nHmm, yeah, I think that's it. It makes sense that her bladder's not working quite right, given that she's having these symptoms. So, cystometry would likely show that her detrusor contractions are a bit off and that there's a residual volume that's higher than normal. That's pretty much what we'd expect in this situation.\n</think>\nCystometry would most likely reveal that the patient's detrusor contractions are not fully effective, and there is a residual volume that is higher than normal. This suggests that her bladder is not emptying completely, likely due to a functional issue with the detrusor muscle.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"question = \"\"\"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, \n              and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, \n              gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. \n              What is the most likely predisposing factor for this patient's condition?\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T22:05:37.029356Z","iopub.execute_input":"2025-06-19T22:05:37.029668Z","iopub.status.idle":"2025-06-19T22:06:09.964732Z","shell.execute_reply.started":"2025-06-19T22:05:37.029648Z","shell.execute_reply":"2025-06-19T22:06:09.963989Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nAlright, let's think about this. We have a 59-year-old man with some classic symptoms of an infection—fever, chills, night sweats, and fatigue. And there's this vegetation on his aortic valve, which really catches my attention. That makes me think about endocarditis, specifically valvular endocarditis. \n\nOkay, so now let's look at the microbiology details. Blood cultures show gram-positive, catalase-negative, gamma-hemolytic cocci in chains that don't grow in a 6.5% NaCl medium. Hmm, that's interesting. These characteristics remind me of something I've heard about before. I think it's called viridans group streptococcus or maybe something similar.\n\nOh wait, viridans group streptococcus! That's right. It's known for causing endocarditis, especially on the aortic valve. Now, what's the big risk factor for this kind of infection? I remember that it's often related to some kind of underlying heart condition or maybe even a previous heart valve issue. \n\nOh, and I just thought of something else. There's this thing called the Duke's criterion, which helps determine if an infection is due to endocarditis. It looks at things like if the patient has a history of heart valve problems, like maybe they've had surgery or a previous valve replacement. That could be a big clue. \n\nSo, putting it all together, this guy's symptoms and the specific type of bacteria in his blood make me think of viridans group streptococcus endocarditis. And the most likely reason this might have happened is probably due to some kind of underlying heart condition or a previous valve issue, like maybe he's had a valve replacement before or has a heart valve problem. \n\nYeah, I think that's the direction we're going with this. It's all pointing towards an underlying heart condition being the main thing that's leading to this infection. Okay, I feel pretty confident about this now.\n</think>\nThe patient’s symptoms and the presence of a 12 mm vegetation on the aortic valve, combined with the identification of gram-positive, catalase-negative, gamma-hemolytic cocci in chains, are indicative of viridans group streptococcus endocarditis. Viridans group streptococcus is known for causing endocarditis, particularly on the aortic valve. The most likely predisposing factor for this condition is an underlying heart condition, such as a previous heart valve issue or a history of heart valve surgery or replacement, which increases the risk of infection.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":46}]}